---
title: "Intro to DS - LM part II factor regressors"
author: "Edwin Lo, GWU Intro to Data Science DATS 6101"
# date: "today"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r init, include=FALSE}
# some of common options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
library(ezids)
library(ggplot2)
library(gridExtra)
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
options(scientific=T, digits = 3) 
# options(scipen=9, digits = 3) 
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times
```

# HW assignment

## Linear model - categorical regressors 

Let us re-analyze the problem we had from last time, but include categorical regressors. First, 
import the data, and change those appropriate ones to factors.

```{r}
bikeorig = data.frame(read.csv("bikedata.csv"))
bike = subset(bikeorig, select = -c(Date, Casual.Users, Registered.Users)) # remove irrelevant columns
colnames(bike)[4:11] = c("Day","Workday","Weather","TempF","TempxFF","Humidity","Wind","Tusers") # rename some columns
bike16 = subset(bike, bike$Hour == 16) # with only Hour-16 data. All columns are numerical
nrow(bike16)
bike16$Hour = NULL # Hour has only one value '16' now. No need to keep this column.
bike16$TempxFF = NULL 
bike_final = bike16
bike_final$Season = factor(bike16$Season)
bike_final$Holiday = factor(bike16$Holiday)
bike_final$Day = factor(bike16$Day)
bike_final$Workday = factor(bike16$Workday)
bike_final$Weather = factor(bike16$Weather)
str(bike_final) # Same as bike16 except some columns are now factor level.
```

### Question 0  
**Pearson vs Spearman**  
Read the article here:  
Hauke,J. & Kossowski,T.(2011). [Comparison of Values of Pearson's and Spearman's 
Correlation Coefficients on the Same Sets of Data. 
Quaestiones Geographicae, 30(2) 87-93](https://doi.org/10.2478/v10117-011-0021-1).  
Simply indicate you have read it.  

I read the article and understood that Spearman correlation is based on rank statistic and is primarily used when:  

1. Assumption of the normality does not necessarily need to be made  
2. You want a monotonic function to describe the relationship between the two variables rather just a linear function  
3. Qualitative variables required to be assessed  

### Question 1  
**Compare the difference of the correlation matrix between the Pearson and Spearman methods**  
Look at the correlation matrices using the two methods. Compare and comment on their differences.  

```{r, fig.width=11, fig.height=4}

num_cnames <- NULL
for(cname in colnames(bike_final))
{
  if(class(bike_final[, cname]) != "factor")
  {
    num_cnames <- c(num_cnames, cname)
  }
}

pearson <- cor(bike_final[, num_cnames])
spearman <- cor(bike_final[, num_cnames], method = 'spearman')

pearson[lower.tri(pearson)] <- NA
spearman[lower.tri(spearman)] <- NA


library(reshape2)
pearson <- melt(pearson, na.rm=T)
spearman <- melt(spearman, na.rm=T)

# Heatmap
library(ggplot2)
p <- ggplot(data = pearson, aes(Var2, Var1, fill = value))+
     geom_tile(color = "white")+
     xlab('Vars')+
     ylab('Vars')+
     geom_text(aes(Var2, Var1, label = round(value, 3)), color = "black", size = 4) +
     scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
       midpoint = 0, limit = c(-1,1), space = "Lab", 
       name="Pearson\nCorrelation") +
      theme_minimal()+ 
     theme(axis.text.x = element_text(angle = 45, vjust = 1, 
        size = 12, hjust = 1))+
     coord_fixed()

s <- ggplot(data = spearman, aes(Var2, Var1, fill = value))+
     geom_tile(color = "white")+
     xlab('Vars')+
     ylab('Vars')+
     geom_text(aes(Var2, Var1, label = round(value, 3)), color = "black", size = 4) +
     scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
       midpoint = 0, limit = c(-1,1), space = "Lab", 
       name="Spearman\nCorrelation") +
      theme_minimal()+ 
     theme(axis.text.x = element_text(angle = 45, vjust = 1, 
        size = 12, hjust = 1))+
     coord_fixed()

grid.arrange(p, s, nrow=1)

```

Spearman is a modified version of Pearson correlation that converts the data into rank to estimate how well a monotonic function can describe the relationship between two variables, as per the paper by Hauke et.al. For instance, let me first convert Humidity and TempF to rank so that we can then use Pearson to compute the correlation to compare.  

```{r, results='markup'}
humidity <- rank(bike_final$Humidity)
temp <- rank(bike_final$TempF)
cor(data.frame(Humidity=humidity, TempF=temp))
```

I can clearly see the above result is consistent with the Spearman correlation plot.  

### Question 2    
**Build a baseline linear model for `Tusers`, with one numerical predictor.**  
Write down the model equation as we did in class, like  
`Tusers` = 0.28 + 5.29 `TempF`, just as an example.
```{r, results='markup'}
model1 <- lm(Tusers~TempF, data=bike_final)
summary(model1)
Tusers <- -0.154 + 4.828 * bike_final$TempF

temp <- seq(min(bike_final$TempF), max(bike_final$TempF), length.out=100)
df2 <- data.frame(TempF=temp)
Tusers <- predict.lm(model1, df2)
df2 <- data.frame(TempF=temp, Tusers=Tusers)
ggplot(bike_final, aes(x=TempF, y=Tusers)) +
geom_point(color='blue') +
geom_line(data=df2, color='red', lwd=1.1)
```
`Tusers` = - 0.154 + 4.828 `TempF`  

### Question 3    
**Build a linear model for `Tusers`, with one numerical and one categorical predictor with at least 3 levels.**  
Use the correlation matrix as a guide to decide on which variables to use. 
Find and interpret the results. Also write down the model equation for the different 
categorical factor levels on separate lines.

In order to build a linear model with one numerical and a categorical variable with correlation into account, I believe it would be appropriate to first look at the correlation between all variables including numerical and categorical by converting all of them into rank order.  
```{r}

bike_final2 <- bike_final
bike_final2$Season <- rank(bike_final2$Season)
bike_final2$Holiday <- rank(bike_final2$Holiday)
bike_final2$Day <- rank(bike_final2$Day)
bike_final2$Workday <- rank(bike_final2$Workday)
bike_final2$Weather <- rank(bike_final2$Weather)
bike_final2$TempF <- rank(bike_final2$TempF)
bike_final2$Humidity <- rank(bike_final2$Humidity)
bike_final2$Wind <- rank(bike_final2$Wind)
bike_final2$Tusers <- rank(bike_final2$Tusers)

corrplot::corrplot(cor(bike_final2),
                   method='number',
                   type='lower')

```

I think we are now in a good shape to create a linear model with one continuous and a categorical variable with at least 3 levels. The categorical variable that is next highly correlated with Tusers would be Season (with 4 levels).  

```{r, results='markup'}
model2 <- lm(Tusers~TempF+Season, data=bike_final)
summary(model2)

s2_b <- bike_final$Season == 2
s3_b <- bike_final$Season == 3
s4_b <- bike_final$Season == 4

tusers <- -45.197 + 5.576*bike_final$TempF - 55.725*bike_final$s2_b + 59.523*bike_final$s3_b -16.011*bike_final$s4_b
```

Since season is a categorial variable with 4 levels, 3=n.levels-1 coefficients are used along with TempF to make the final prediction. If Season2, Season3 and Season4 all are False, then it is assumed that Season1 must be activated and this is adjusted in the Intercept term.  

If Season1 then `Tusers` = -45.197 + 5.576 `TempF`  
   Season2 then `Tusers` = -45.197 + 5.576 `TempF` - 55.725 `Season2`
   Season3 then `Tusers` = -45.197 + 5.576 `TempF` + 59.523 `Season3`
   Season4 then `Tusers` = -45.197 + 5.576 `TempF` - 16.011 `Season4`

### Question 4  
**Next extend the previous model for `Tusers`, but include the interaction term between the numerical and categorical variable.**  
Again, write down the model equation for different categorical factor levels on separate lines. Comment of the slope and coefficients. 

"But" in the line - extend the model "but" include the interaction..., is slightly confusing.  
Again, "slope and coefficients" ? You mean bias/intercept and coefficients/slopes ?  

```{r, results='markup'}
model3 <- lm(Tusers~TempF+Season+TempF:Day, data=bike_final)
summary(model3)
```

If Day0 and  
   Season1 then `Tusers` = -59.437 + 6.575 `TempF`
   Season2 then `Tusers` = -59.437 + 6.575 `TempF` - 60.758 `Season2`
   Season3 then `Tusers` = -59.437 + 6.575 `TempF` + 59.037  `Season3`
   Season4 then `Tusers` = -59.437 + 6.575 `TempF` - 12.502 `Season4`
   
If Day1 and  
   Season1 then `Tusers` = -59.437 + 6.575 `TempF`                    - 1.177 `TempF`*`Day1`  
   Season2 then `Tusers` = -59.437 + 6.575 `TempF` - 60.758 `Season2` - 1.177 `TempF`*`Day1`  
   Season3 then `Tusers` = -59.437 + 6.575 `TempF` + 59.037 `Season3` - 1.177 `TempF`*`Day1`  
   Season4 then `Tusers` = -59.437 + 6.575 `TempF` - 12.502 `Season4` - 1.177 `TempF`*`Day1`  

If Day2 and  
   Season1 then `Tusers` = -59.437 + 6.575 `TempF`                    - 1.211 `TempF`*`Day2`  
   Season2 then `Tusers` = -59.437 + 6.575 `TempF` - 60.758 `Season2` - 1.211 `TempF`*`Day2`  
   Season3 then `Tusers` = -59.437 + 6.575 `TempF` + 59.037 `Season3` - 1.211 `TempF`*`Day2`  
   Season4 then `Tusers` = -59.437 + 6.575 `TempF` - 12.502 `Season4` - 1.211 `TempF`*`Day2`  
   
If Day3 and  
   Season1 then `Tusers` = -59.437 + 6.575 `TempF`                    - 1.412 `TempF`*`Day3`  
   Season2 then `Tusers` = -59.437 + 6.575 `TempF` - 60.758 `Season2` - 1.412 `TempF`*`Day3`  
   Season3 then `Tusers` = -59.437 + 6.575 `TempF` + 59.037 `Season3` - 1.412 `TempF`*`Day3`  
   Season4 then `Tusers` = -59.437 + 6.575 `TempF` - 12.502 `Season4` - 1.412 `TempF`*`Day3`  
   
If Day4 and  
   Season1 then `Tusers` = -59.437 + 6.575 `TempF`                    - 1.254 `TempF`*`Day4`  
   Season2 then `Tusers` = -59.437 + 6.575 `TempF` - 60.758 `Season2` - 1.254 `TempF`*`Day4`  
   Season3 then `Tusers` = -59.437 + 6.575 `TempF` + 59.037 `Season3` - 1.254 `TempF`*`Day4`  
   Season4 then `Tusers` = -59.437 + 6.575 `TempF` - 12.502 `Season4` - 1.254 `TempF`*`Day4`  
   
If Day5 and  
   Season1 then `Tusers` = -59.437 + 6.575 `TempF`                    - 0.491 `TempF`*`Day5`  
   Season2 then `Tusers` = -59.437 + 6.575 `TempF` - 60.758 `Season2` - 0.491 `TempF`*`Day5`  
   Season3 then `Tusers` = -59.437 + 6.575 `TempF` + 59.037 `Season3` - 0.491 `TempF`*`Day5`  
   Season4 then `Tusers` = -59.437 + 6.575 `TempF` - 12.502 `Season4` - 0.491 `TempF`*`Day5`  

If Day6 and  
   Season1 then `Tusers` = -59.437 + 6.575 `TempF`                    - 0.184 `TempF`*`Day6`  
   Season2 then `Tusers` = -59.437 + 6.575 `TempF` - 60.758 `Season2` - 0.184 `TempF`*`Day6`  
   Season3 then `Tusers` = -59.437 + 6.575 `TempF` + 59.037 `Season3` - 0.184 `TempF`*`Day6`  
   Season4 then `Tusers` = -59.437 + 6.575 `TempF` - 12.502 `Season4` - 0.184 `TempF`*`Day6`  

The intercept accounts for both Day0 and Season1, while the values of the interaction term has an opposite effect on the response variable i.e., they are negatively correlated.  

### Question 5  
**Let us use this model equation `Tusers ~ TempF + Season + Wind:Weather + Season:Weather`.**  
Notice the presence/absence of coefficients for the base-level categories. 
No need to write down the model equations this time. But comment on what is the difference 
between how the base-level is handled here and the previous models. 

```{r, results='markup'}
tmp_model <- lm(Tusers ~ TempF + Season + Wind:Weather + Season:Weather, data=bike_final)
summary(tmp_model)
```

For this particular model, I am able to see `Wind`:`Weather1`, whereas in the previous models, the coefficient for the first level of a categorical variable is always adjusted in the intercept. There is also singularity in this model that I have explained in the last question.  


### Question 6  
**Compare the above models using ANOVA**  
Interpret and comment on your results. 
```{r, results='markup'}
anova.model <- anova(model1, model2, model3)
anova.model
```
 
The consequential expansion of the model shows statistical significance between their residuals. In other words, the model has improved. 

### Question 7   
**Try build a model with three categorical variables only, and their interaction terms.**  
What are we really getting?  Describe and explain. 
  
```{r, results='markup'}
last_model <- lm(Tusers~(Season+Weather+Workday)^2, data=bike_final)
summary(last_model)
```

From the lectures in the class, OneNote materials, and from what I can also understand on what's given in [analysis factor](https://www.theanalysisfactor.com/interpreting-interactions-in-regression/#:~:text=Adding%20interaction%20terms%20to%20a,each%20coefficient%20is%20telling%20you.), I believe without the interaction term included, we interpret `Season2` as the unique effect of Season2 on `Tusers`. When an interaction term is included such as `Season2`:`Weather2`, we are controlling one of the predictors to assess the effect of the other predictor on the response variable for different values of the controlled variable. The term controlled variable is used to mean that by keeping, for instance, `Weather2=True`, we are assessing the effect of `Season2` when `Weather2=True`. It is also understood that interaction terms are typically included to testing our hypothesis on how one predictor would affect the response when one of the other values of the predictor variable is held constant. From what's given in the OneNotes, I can also understand that using interaction terms can be used to filter significant terms from the summary of the model. Perhaps a new LM can be created including those that are significant.  

Relating to the NA's as coefficients for some of the interaction terms, I found from [stackexchange](https://stats.stackexchange.com/questions/25804/why-would-r-return-na-as-a-lm-coefficient) that the multicollinearity is the cause for this behavior. Some of the interactions i.e., that particular vector resulting from the interaction in the vector space is considered a redundant information as it can be obtained as a combination of other vectors and this is a common problem while computing closed form solution for regression. I believe this the reason system also warns that "4 variables not defined because of singularities".  
