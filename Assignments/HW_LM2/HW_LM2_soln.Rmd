---
title: "Intro to DS - LM part II factor regressors"
author: "Edwin Lo, GWU Intro to Data Science DATS 6101"
# date: "today"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r init, include=FALSE}
# some of common options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
library(ezids)
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
options(scientific=T, digits = 3) 
# options(scipen=9, digits = 3) 
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times
```

# HW assignment

## Linear model - categorical regressors 

Let us re-analyze the problem we had from last time, but include categorical regressors. First, 
import the data, and change those appropriate ones to factors.

```{r}
bikeorig = data.frame(read.csv("bikedata.csv"))
bike = subset(bikeorig, select = -c(Date, Casual.Users, Registered.Users)) # remove irrelevant columns
colnames(bike)[4:11] = c("Day","Workday","Weather","TempF","TempxFF","Humidity","Wind","Tusers") # rename some columns
bike16 = subset(bike, bike$Hour == 16) # with only Hour-16 data. All columns are numerical
nrow(bike16)
bike16$Hour = NULL # Hour has only one value '16' now. No need to keep this column.
bike16$TempxFF = NULL 
bike_final = bike16
bike_final$Season = factor(bike16$Season)
bike_final$Holiday = factor(bike16$Holiday)
bike_final$Day = factor(bike16$Day)
bike_final$Workday = factor(bike16$Workday)
bike_final$Weather = factor(bike16$Weather)
str(bike_final) # Same as bike16 except some columns are now factor level.
```

### Question 0  
**Pearson vs Spearman**  
Read the article here:  
Hauke,J. & Kossowski,T.(2011). [Comparison of Values of Pearson's and Spearman's 
Correlation Coefficients on the Same Sets of Data. 
Quaestiones Geographicae, 30(2) 87-93](https://doi.org/10.2478/v10117-011-0021-1).  
Simply indicate you have read it.

### Question 1  
**Compare the difference of the correlation matrix between the Pearson and Spearman methods**  
Look at the correlation matrices using the two methods. Compare and comment on their differences.

```{r, results='markup'}
loadPkg("corrplot")
corPearson = cor(bike16)
# notice that cor(bike_final) does not work, since the cor() function does not handle factor variable.
corrplot.mixed(corPearson)
xkabledply(corPearson, title = "Correlation Matrix for bikeshare df, Pearson (Hour==16 subset, all variables)")

corSpearman = cor(bike16, method = "spearman")
# notice that cor(bike_final) does not work, since the cor() function does not handle factor variable.
corrplot.mixed(corSpearman)
xkabledply(corSpearman, title = "Correlation Matrix for bikeshare df, Spearman (Hour==16 subset, all variables)")

unloadPkg("corrplot")
```

A quick inspection of the two matrices find there are some slight differences 
between the two. To further quantify the effect, let us look at the percent difference in a matrix form: 


```{r, results="markup"}
corDiff = corPearson - corSpearman
xkabledply(corDiff, title = "Difference between the Pearson and Spearman methods")
corDiffP = 100*corDiff / (corPearson + corSpearman)
xkabledply(corDiffP, title = "Percent Difference between the Pearson and Spearman methods")

```

The differences between the two varies quite a bit. they range from 
`r min(corDiff)` (between Tusers and Humidity, both numerical variables) 
to `r max(corDiff)` (between Weather and Humidity, categorical and numerical/int).  

As for the percent differences, they range from 
`r min(corDiffP)` (between Weather and Workday, both categorical variables) 
to `r max(corDiffP)` (between TempF and Wind, both numerical/int).  

I do not observe any pattern or rules for such differences. It seems to be all 
case-specific.


### Question 2    
**Build a baseline linear model for `Tusers`, with one numerical predictor.**  
Write down the model equation as we did in class, like  
`Tusers` = 0.28 + 5.29 `TempF`, just as an example.

```{r, results='asis'}
model1 = lm(Tusers ~ TempF, data=bike_final)
sum_md1 = summary(model1)  # this is easier to be used in the inline codes to pull out coefficients and other info
xkabledply(model1)
```

The regression model equation is given by the intercept and coefficient above:  
`Tusers` =  `r format(sum_md1$coefficients['(Intercept)','Estimate'])` + `r format(sum_md1$coefficients['TempF','Estimate'])` ⨉ `TempF`   
The 

* The intercept of `r format(sum_md1$coefficients['(Intercept)','Estimate'])` indicates the predicted number of users even if we extrapolate the `TempF` to zero.  
* The coefficient for `TempF` indicates the model predicts `r format(sum_md1$coefficients['TempF','Estimate'])` more users per degree increase in `TempF`, a positive effect.  
* The p-values for `TempF` coefficient (`r format(sum_md1$coefficients['TempF','Pr(>|t|)']) `) is extremely small, so we are very confident `Tusers` has a strong positive 
correlation with `TempF`.
* The p-value for the intercept (`r format(sum_md1$coefficients['(Intercept)','Pr(>|t|)'])`) is very high. However it is not that important if we know what is the predicted `Tusers` at temperature zero to begin with. 
* The $R^2$ value indicates the model can explain `r format(sum_md1$r.squared *100)`% of the variation in `Tusers` from the variations in `TempF`.


### Question 3    
**Build a linear model for `Tusers`, with one numerical and one categorical predictor with at least 3 levels.**  
Use the correlation matrix as a guide to decide on which variables to use. 
Find and interpret the results. Also write down the model equation for the different 
categorical factor levels on separate lines.

```{r, results='asis'}
model2 = lm(Tusers ~ TempF+Season, data=bike_final)
sum_md2 = summary(model2)  # this is easier to be used in the inline codes to pull out coefficients and other info
xkabledply(model2)
```

The regression model equation is given by the intercept and coefficients above summarized as:  

* Season1: `Tusers` =  `r format(sum_md2$coefficients['(Intercept)','Estimate'])` + `r format(sum_md2$coefficients['TempF','Estimate'])` ⨉ `TempF`   
* Season2: `Tusers` =  `r format(sum_md2$coefficients['(Intercept)','Estimate'])` + `r format(sum_md2$coefficients['Season2','Estimate'])` + `r format(sum_md2$coefficients['TempF','Estimate'])` ⨉ `TempF`   
* Season3: `Tusers` =  `r format(sum_md2$coefficients['(Intercept)','Estimate'])` + `r format(sum_md2$coefficients['Season3','Estimate'])` + `r format(sum_md2$coefficients['TempF','Estimate'])` ⨉ `TempF`   
* Season4: `Tusers` =  `r format(sum_md2$coefficients['(Intercept)','Estimate'])` + `r format(sum_md2$coefficients['Season4','Estimate'])` + `r format(sum_md2$coefficients['TempF','Estimate'])` ⨉ `TempF`   

Essentially, the four seasons have a different starting point as the intercept for the model. 

Also notice that the p-value for Season4 coefficient (`r format(sum_md2$coefficients['Season4','Pr(>|t|)'])`) is very high. 

The $R^2$ value is `r format(sum_md2$r.squared)`.


### Question 4  
**Next extend the previous model for `Tusers`, but include the interaction term between the numerical and categorical variable.**  
Again, write down the model equation for different categorical factor levels on separate lines. Comment of the slope and coefficients. 

```{r, results='asis'}
model3 = lm(Tusers ~ TempF * Season, data=bike_final)
sum_md3 = summary(model3)  # this is easier to be used in the inline codes to pull out coefficients and other info
xkabledply(model3)
```

The regression model equation is given by the intercept and coefficients above summarized as:  

* Season1: `Tusers` =  `r format(sum_md3$coefficients['(Intercept)','Estimate'])` + `r format(sum_md3$coefficients['TempF','Estimate'])` ⨉ `TempF`   
* Season2: `Tusers` =  (`r format(sum_md3$coefficients['(Intercept)','Estimate'])` + `r format(sum_md3$coefficients['Season2','Estimate'])` ) + ( `r format(sum_md3$coefficients['TempF','Estimate'])`  + `r format(sum_md3$coefficients['TempF:Season2','Estimate'])` ) ⨉ `TempF`   
* Season3: `Tusers` =  (`r format(sum_md3$coefficients['(Intercept)','Estimate'])` + `r format(sum_md3$coefficients['Season3','Estimate'])` ) +  ( `r format(sum_md3$coefficients['TempF','Estimate'])`  + `r format(sum_md3$coefficients['TempF:Season3','Estimate'])` ) ⨉ `TempF`  
* Season4: `Tusers` =  (`r format(sum_md3$coefficients['(Intercept)','Estimate'])` + `r format(sum_md3$coefficients['Season4','Estimate'])` ) +  ( `r format(sum_md3$coefficients['TempF','Estimate'])`  + `r format(sum_md3$coefficients['TempF:Season4','Estimate'])` ) ⨉ `TempF`  

Essentially, the four seasons have a different starting intercept AS WELL AS different slopes for `TempF` for the linear models. 

The $R^2$ value is `r format(sum_md3$r.squared)`.


### Question 5  
**Let us use this model equation `Tusers ~ TempF + Season + Wind:Weather + Season:Weather`.**  
Notice the presence/absence of coefficients for the base-level categories. 
No need to write down the model equations this time. But comment on what is the difference 
between how the base-level is handled here and the previous models. 

```{r, results='asis'}
model4 = lm(Tusers ~ TempF + Season + Wind:Weather + Season:Weather, data=bike_final)
sum_md4 = summary(model4)  # this is easier to be used in the inline codes to pull out coefficients and other info
xkabledply(model4)
```

The interaction term between `Wind`:`Weather` includes a coefficient for the base-level unlike the previous cases. This is because 
there is no `Wind` term by itself, so the base level for `Wind:Weather1` has nowhere to 
combine or hide that coefficient.  

In general, when we have two categorical variables interacting, we will be getting 
a combinations of the different levels, and adjust the **intercepts" accordingly 
for the different combo. 

The $R^2$ value is `r format(sum_md4$r.squared)`.


### Question 6  
**Compare the above models using ANOVA**  
Interpret and comment on your results. 

 
```{r}
aovresult <- anova(model1, model2, model3, model4)
aovresult
```

Comparing them using ANOVA, the result shows that the models are improving as we 
add more and more coefficients in a significant way.  Another quick summary:  

* Model 1 : `r format(formula(model1))`  , $R^2$ = `r format(sum_md1$r.squared)`
* Model 2 : `r format(formula(model2))`  , $R^2$ = `r format(sum_md2$r.squared)`  
* Model 3 : `r format(formula(model3))`  , $R^2$ = `r format(sum_md3$r.squared)`  
* Model 4 : `r format(formula(model4))`  , $R^2$ = `r format(sum_md4$r.squared)`  

```{r, results="markup"}
xkabledply(aovresult)
```



### Question 7   
**Try build a model with three categorical variables only, and their interaction terms.**  
What are we really getting?  Describe and explain. 
  

```{r}
model5 = lm(Tusers ~ Workday*Weather*Season, data=bike_final)
sum_md5 = summary(model5)
xkabledply(model5)
```

The "model" is basically giving us for each combination of different levels, for example, 
when `Workday` = 2, `Season` = 3, `Weather` = 4, what is the average `Tuser` value. 
In other words, we are getting the "null models" (with no slope/coefficient) 
of the average values for specific levels. 






