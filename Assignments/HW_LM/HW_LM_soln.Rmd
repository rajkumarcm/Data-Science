---
title: "Intro to DS - Linear Model part I"
author: "Edwin Lo, GWU Intro to Data Science DATS 6101"
# date: "today"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r init, include=FALSE}
# some of common options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
library(ezids)
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
options(scientific=T, digits = 3) 
# options(scipen=9, digits = 3) 
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times
```

# HW assignment

## Linear model - quantitative regressors 

### Question 1  
**Import the data, call it `bikeorig.`**  
The `Date` variable is probably imported as factor level variable. In any case, let us remove `Date`, `Casual.Users`, 
and `Registered.Users` in the dataset and save it as a new datafame, call it `bike`. How many variables are in `bike`? 
How many of them are imported as `int`? Feel free to rename longer variable names into shorter ones for convenience.  

```{r}
bikeorig = data.frame(read.csv("bikedata.csv"))
# head(bikeorig)
str(bikeorig)
```
```{r}
bike = subset(bikeorig, select = -c(Date, Casual.Users, Registered.Users))
str(bike)
colnames(bike)[4:11] = c("Day","Workday","Weather","TempF","TempFF","Humidity","Wind","Tusers")
str(bike)
```

There are `r ncol(bike)` variables in bike. A quick look at the structure shows that other than `Temperature.F` and `Temperature.Feels.F` imported as *num*, all `r ncol(bike) -2` other variables are imported as *int*.

Several columns with long names have been also been changed to shorter ones. They were changed from (`r colnames(bikeorig)[4:11]`, `r colnames(bikeorig)[14]`) into (`r colnames(bike)[4:11]`).


### Question 2    
**Select only the subset with `Hour` equal 16 only. Call it `bike16`**  
These are the afternoon rush hour data. How many observations are there?   

```{r}
bike16 = subset(bike, bike$Hour == 16)
nrow(bike16)
bike16$Hour = NULL # Hour has only one value '16' now. No need to keep this column.
```

There are `r nrow(bike16)` observations in the `Hour==16` subset. Since `Hour` has only one value in the datqframe now, there is no reason to keep it as a predictor. We dropped it from here onwards.


### Question 3  
**Before building any models, we should make sure the variables are set up properly.**  
(This problem is solved for you. Codes are given below.)  
Which ones should be recorded as categorical? Convert them now before we proceed to the model building.  

Note: After converting, the correlation function `cor()` will not work with categorical/factor variables. 
I would keep the original `bike16` dataframe as numeric, and use that to 
find the correlation matrix. 
Although technically correlation between categorical and numeric variables are not 
well defined in general, we can still get some useful information if the 
categorical variable is at least at ordinal level. See future discussion 
on using "Pearson" vs "Spearman" methods for correlation tests. 

While the `cor()` function does not accept categorical variables (and therefore 
we cannot use it for `corrplot()`), the `lattice::pairs()` function does not complain 
about categorical columns. We can still use it to get a visual distribution of 
data values from it.
 

```{r}
bike_final = bike16
bike_final$Season = factor(bike16$Season)
#bike_final$Hour = factor(bike16$Hour)
bike_final$Holiday = factor(bike16$Holiday)
bike_final$Day = factor(bike16$Day)
bike_final$Workday = factor(bike16$Workday)
bike_final$Weather = factor(bike16$Weather)
str(bike_final)
```

We decided to convert these variables into categorical (factor):  
`Season`, `Holiday`, `Day`, `Workday`, and `Weather`.  Notice that 
the dataframe `bike16` still has all variables numerical, while the df `bike_final` 
include categorical columns that we just converted. 


### Question 4  
**Make a `pairs()` plot with all the variables (quantitative and qualitative).**  
The `paris` plot on the new dataframe `bike_final` is given here:  

```{r}
loadPkg("lattice") # lattice and ggplot2 combined allow us to use the "pairs" function below 
pairs(bike_final)
unloadPkg("lattice")
```

### Question 5  
**Make a `corrplot()` with only the numerical variables.**  
You can either subset the df with only numerical variables first, then create 
the create the cor-matrix to plot. Or you can create the cor-matrix from 
`bike16`, then select select out the portion of the matrix that you want. 
Use options that shows well the relationships between different variables. 
 
 
Just for testing, we can create the full matrix if we use the original df with 
all numerical variables:
```{r}
loadPkg("corrplot")
corbike16 = cor(bike16)
# we can try that cor(bike_final) does not work, since some columns are now categorical.

# corrplot(corbike16, type = "lower")
# corrplot(corbike16, type = "upper", method = "number")
corrplot.mixed(corbike16)
# unloadPkg("corrplot")
```

You can also try displaying the cor-matrix using `xkabledply()`.

```{r, results="markup"}
xkabledply(corbike16, title = "Correlation Matrix for bikeshare data (Hour==16 subset, all variables)")
```

Follow the instructions, however, we should subset only the numerical variables. 
They are the last five rows/columns. 
```{r}
# loadPkg("corrplot")
corbike = corbike16[6:10,6:10]
# corrplot(corbike, type = "lower")
# corrplot(corbike, type = "upper", method = "number")
corrplot.mixed(corbike)
unloadPkg("corrplot")
```

```{r results='markup'}
xkabledply(corbike, title = "Correlation Matrix for bikeshare data (Hour==16 subset, numerical vars)")
```


### Question 6   
**By using numerical variables only, build a linear model with 1 independent variable to predict the `Total Users`.**  
Choose the variable with the strongest correlation coefficient. Make some short 
comments on the coefficient values, their p-values, and the multiple R-squared value.  

```{r}
model1 = lm(Tusers ~ TempFF, data=bike_final)
sum_md1 = summary(model1) # also for later use on inline codes
sum_md1
```

From the results above, the strongest correlated variable with `Tusers` is either `TempF` or `TempFF`. Obviously, the two are highly correlated among themselves, with correlation coefficient of `r cor(bike_final$TempF,bike_final$TempFF)`. We should only pick at most one of the two in any models. Let us choose `tempFF`. 

```{r results='markup'}
xkabledply(model1, title="Summary of LM `Tusers ~ TempFF`")
```

* The intercept of `r format(sum_md1$coefficients['(Intercept)','Estimate'])` indicates the predicted number of users even if we extrapolate the `TempFF` to zero.  
* The coefficient for `TempFF` indicates the model predicts `r format(sum_md1$coefficients['TempFF','Estimate'])` more users per degree increase in `TempFF`, a positive effect.  
* The p-values for both the intercept (`r format(sum_md1$coefficients['(Intercept)','Pr(>|t|)'])`) and TempFF coefficient (`r format(sum_md1$coefficients['TempFF','Pr(>|t|)']) `) are extremely small. Therefore the coeffeicients are statistically significant.   
* The $R^2$ value indicates the model can explain `r format(sum_md1$r.squared *100)`% of the variation in `Tusers` from the variations in `TempFF`.


### Question 7   
**Next, add a second variable to the model.**  
Choose the variable with the next strongest correlation, but avoid using obviously 
collinear variables. When you have the model, 
check the VIF values. If the VIF is higher than 5, discard this model, and try the 
variable with the next strongest correlation until you find one that works 
(ideally with vif’s <5, or if you have to, allow vif up to 10). Again, 
comment on the coefficient values, their p-values, 
and the multiple R-squared value.  

As we explained before, we would only choose either `TempFF` or `TempF`. As a result, the next numerical variable to choose would be `Humidity` as the next highest correlated variable with `Tusers` at *r* = `r format( cor( bike_final$Tusers, bike_final$Humidity ) )`. So we build the 2-variable model, and obtained the following:
```{r}
model2 = lm(Tusers ~ TempFF + Humidity, data=bike_final)
sum_md2 <- summary(model2)
sum_md2
loadPkg("faraway")
vif_md2 = faraway::vif(model2)
vif_md2
```

The summary tables of the model and vif values are given here:  
```{r, results="markup"}
xkabledply(model2, title="Summary of LM `Tusers ~ TempFF + Humidity`")
xkablevif(model2, title="VIFs of the model `Tusers ~ TempFF + Humidity`")
```

Some notable observations:  
* The VIF values of the model for the variables `r names(vif_md2)` are `r vif_md2` respectively, well under 5. Multicollinearity is minimal in this model.   
* The intercept of `r format(sum_md2$coefficients['(Intercept)','Estimate'])` indicates the predicted number of users even if we extrapolate the `TempFF` and `Humidity` to zero.  
* The coefficient for `TempFF` indicates the model predicts `r format(sum_md2$coefficients['TempFF','Estimate'])` more users per degree increase in `TempFF`, a positive effect, **given** that the other variable `Humidity` is unchanged.  
* The coefficient for `Humidity` indicates the model predicts `r format(sum_md2$coefficients['Humidity','Estimate'])` more users per unit increase in `Humidity`, a negative effect, **given** that the other variable `TempFF` is unchanged.  
* The $R^2$ value indicates the model can explain `r format(sum_md2$r.squared *100)`% of the variation in `Tusers` from the variations in `TempFF` and `Humidity`.


### Question 8  
**We will try one more time as in the previous question, to add a third variable in our model.**  

Well, the only other numerical variable available is `Wind` here.  
```{r}
model3 = lm(Tusers ~ TempFF +  Humidity + Wind, data=bike_final)
sum_md3 <- summary(model3)
sum_md3
vif_md3 = faraway::vif(model3)
vif_md3
```

The summary tables of the model and vif values are given here:  
```{r, results="markup"}
xkabledply(model3, title="Summary of linear model `Tusers ~ TempFF + Humidity + Wind`")
xkablevif(model3,title="VIFs of the model `Tusers ~ TempFF + Humidity + Wind`")
```

Some observations:  
* The VIF values are still well under 5. Multicollinearity is minimal in this model.   
* The coefficient for `TempFF` indicates the model predicts `r format(sum_md3$coefficients['TempFF','Estimate'])` more users per degree increase in `TempFF`, a positive effect, **given** that the other variables `Humidity` and `Wind` are unchanged.  
* The coefficient for `Humidity` indicates the model predicts `r format(-sum_md3$coefficients['Humidity','Estimate'])` less users per unit increase in `Humidity`, a negative effect, **given** that the other variables `TempFF` and `Wind` are unchanged.  
* The coefficient for `Humidity` indicates the model predicts `r format(-sum_md3$coefficients['Wind','Estimate'])` less users per unit increase in `Wind`, a negative effect, **given** that the other variables `TempFF` and `Humidity` are unchanged.  
* The $R^2$ value indicates the model can explain `r format(sum_md3$r.squared *100)`% of the variation in `Tusers` from the variations in `TempFF`, `Weather` and `Season`.



### Question 9  
**For the 3-variable model you found, find the confidence intervals of the coefficients.**  

```{r}
cints = confint(model3)
cints # cints here is an 8x2 matrix, index/row are '(Intercept)', 'TempFF', 'Humidity', 'Wind'
```

The confidence intervals (at 95%) are:

*  `intercept`: [`r format( cints['(Intercept)',1:2] )`]  
*  `TempFF`: [`r format( cints['TempFF',1:2] )`]  
*  `Humidity`: [`r format( cints['Humidity',1:2] )`]  
*  `Wind`: [`r format( cints['Wind',1:2] )`]  

Note that for those intervals (`Weather2` and `Weather4`) including zeros are statistically insignificant (at 95% level).  


### Question 10    
**Use ANOVA to compare the three different models you found.**  
You have found three different models. Use ANOVA test to compare their residuals. What conclusion can you draw?


```{r}
aovresult <- anova(model1, model2, model3)
aovresult
```

Comparing them using ANOVA, which compares the residual values predicted in each model for the data points in the dataframe, gives the following result:  

* Model 1 : `r format(formula(model1))`  
* Model 2 : `r format(formula(model2))`  
* Model 3 : `r format(formula(model3))`  

```{r, results="markup"}
xkabledply(aovresult)
```

It shows that model2 is significantly better than model1, in terms of Residual-sum-of-squares (RSS) improvements as well as an extremely small p-value. And model3 is a lot better than model2 statistical-wise, with improved RSS and small p-value (3.65%).

