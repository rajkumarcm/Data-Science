---
title: "Intro to DS - Linear Model part I"
author: "Edwin Lo, GWU Intro to Data Science DATS 6101"
# date: "today"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r init, include=FALSE}
# some of common options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
library(ezids)
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
options(scientific=T, digits = 3) 
# options(scipen=9, digits = 3) 
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times
```

# HW assignment

## Linear model - quantitative regressors 

### Question 1  
**Import the data, call it `bikeorig.`**  
The `Date` variable is probably imported as factor level variable. In any case, let us remove `Date`, `Casual.Users`, 
and `Registered.Users` in the dataset and save it as a new datafame, call it `bike`. How many variables are in `bike`? 
How many of them are imported as `int`? Feel free to rename longer variable names into shorter ones for convenience.  
```{r}
bikeorig = data.frame(read.csv('bikedata.csv', sep=',',
                                header=T, fill=T))
bike <- bikeorig[, c(-1, -12, -13)]
cnames <- colnames(bike)
cnames[4] <- 'Day'
cnames[5] <- 'Workday'
cnames[6] <- 'Weather'
colnames(bike) <- cnames

n_int <- 0
for(cname in colnames(bike)){
  if(class(bike[, cname]) == 'integer')
  {
    n_int = n_int + 1
  }
}

```
There are `r length(colnames(bike))` variables in the bike dataframe and out of those, `r n_int` were imported as an integer.


### Question 2    
**Select only the subset with `Hour` equal 16 only. Call it `bike16`**  
These are the afternoon rush hour data. How many observations are there?   
```{r}
bike16 <- bike[bike[, 'Hour']==16,]
```
There are `r nrow(bike16)` observations in the rush hour subset.

### Question 3  
**Before building any models, we should make sure the variables are set up properly.**  
(This problem is solved for you. Codes are given below.)  
Which ones should be recorded as categorical? Convert them now before we proceed to the model building.  

Note: After converting, the correlation function `cor()` will not work with categorical/factor variables. 
I would keep the original `bike16` dataframe as numeric, and use that to 
find the correlation matrix. 
Although technically correlation between categorical and numeric variables are not 
well defined in general, we can still get some useful information if the 
categorical variable is at least at ordinal level. See future discussion 
on using "Pearson" vs "Spearman" methods for correlation tests. 

While the `cor()` function does not accept categorical variables (and therefore 
we cannot use it for `corrplot()`), the `lattice::pairs()` function does not complain 
about categorical columns. We can still use it to get a visual distribution of 
data values from it.
 

```{r}
bike_final = bike16
bike_final$Season = factor(bike16$Season)
bike_final$Holiday = factor(bike16$Holiday)
bike_final$Day = factor(bike16$Day)
bike_final$Workday = factor(bike16$Workday)
bike_final$Weather = factor(bike16$Weather)
str(bike_final)
```

We decided to convert these variables into categorical (factor):  
`Season`, `Holiday`, `Day`, `Workday`, and `Weather`.  Notice that 
the dataframe `bike16` still has all variables numerical, while the df `bike_final` 
include categorical columns that we just converted. 

### Question 4  
**Make a `pairs()` plot with all the variables (quantitative and qualitative).**  
```{r}
pairs(bike_final)
```
### Question 5  
**Make a `corrplot()` with only the numerical variables.**  
You can either subset the df with only numerical variables first, then create 
the create the cor-matrix to plot. Or you can create the cor-matrix from 
`bike16`, then select select out the portion of the matrix that you want. 
Use options that shows well the relationships between different variables. 
```{r}

# num_cols <- NULL
# for(cname in colnames(bike16))
# {
#   if(class(bike16[, cname])=='numeric')
#   {
#     num_cols <- c(num_cols, cname)
#   }
# }
# I am confused when you asked only to use numeric variables since temp.f and feels like are the only two variables with numerical data type so I  thought you meant to use variables that are non-factors instead of numeric what I interpret as float.  

num_cols <- c('Temperature.F', 'Temperature.Feels.F', 'Humidity', 'Wind.Speed', 'Total.Users')
new_cols <- c('temp.f', 'feel.f', 'humid', 'wind', 'total')

bike16.num_subset <- bike16[, num_cols]
colnames(bike16.num_subset) <- new_cols

bike_cormatrix <- corrplot::corrplot.mixed(cor(bike16.num_subset),
                                     lower = "number",
                                     upper = "circle",
                                     tl.col = "black")
```

I am confused when you asked only to use numeric variables since temp.f and feels like are the only two variables with numerical data type so I thought you meant to use variables that are non-factors instead of numeric what I interpret as float.  

### Question 6   
**By using numerical variables only, build a linear model with 1 independent variable to predict the `Total Users`.**  
Choose the variable with the strongest correlation coefficient. Make some short 
comments on the coefficient values, their p-values, and the multiple R-squared value.  
```{r lm, results='markup'}
model1 <- lm(Total.Users~Temperature.Feels.F, data=bike_final)
summary(model1)
```
**Coefficient interpretation**  
1. An intercept value of `r model1$coefficients[['(Intercept)']]` represents the average number of users when the feel like temperature is $0^{\circ}F$.   
2. A coefficient value for Temperature.Feels.F represents for every increase in this value, the total users increases by `r model1$coefficients[['Temperature.Feels.F']]`  

**Comment on p-value**  
The p value for variable Temperature.Feels.F represents its relationship with variable total users is statistically significant. 

**Comment on Multiple R squared**  
From my understanding, in bivariate linear regression model, both Multiple $R^2$ and $R^2$ are the same. While Multiple R refers to the coefficient of multiple correlation between the independent and the dependent variables, its squared version represents the coefficient of determination.  
The value of 0.309 ***(I couldn't retrieve this dynamically) *** means $\sim 30\%$ of the variance in the dependent variable can be predicted by the independent variable. In other words, it represents how good the model fits the data.  


### Question 7   
**Next, add a second variable to the model.**  
Choose the variable with the next strongest correlation, but avoid using 
obviously collinear variables. When you have the model, check 
the VIF values. If the VIF is higher than 5, discard this model, and try the 
variable with the next strongest correlation until you find one that works 
(ideally with vif’s <5, or if you have to, allow vif up to 10). Again, 
comment on the coefficient values, their p-values, 
and the multiple R-squared value.  
**Temperature.Feels.F vs Temperature.F**
```{r, results='asis'}

vif_values <- ezids::xkablevif(lm(Total.Users ~ Temperature.Feels.F+Temperature.F, data=bike_final[, num_cols]))
vif_values
```
**Temperature.Feels.F vs Humidity**
```{r, results='asis'}
vif_values <- ezids::xkablevif(lm(Total.Users ~ Temperature.Feels.F+Humidity, data=bike_final[, num_cols]))
vif_values
```
**Temperature.Feels.F vs Wind.Speed**
```{r, results='asis'}
vif_values <- ezids::xkablevif(lm(Total.Users ~ Temperature.Feels.F+Wind.Speed, data=bike_final[, num_cols]))
vif_values
```

We can see from the above tables that variables Temperature.Feels.F and Temperature.F have high multicollinearity. Since Humidity has the next strongest correlation coefficient, it is chosen as the second variable for the second model.  

```{r lm2, results='markup'}
model2 <- lm(Total.Users ~ Temperature.Feels.F+Humidity, data=bike_final)
summary(model2)
```
**Coefficient interpretation**  
1. An intercept value of `r model2$coefficients[['(Intercept)']]` represents the average number of users when the feel like temperature and wind speed are both $0^{\circ}F$.   
2. The estimated coefficient value for Temperature.Feels.F represents for every increase in this value, the total users increases by `r model2$coefficients[['Temperature.Feels.F']]`  
3. The estimated coefficient value for Humidity represents for every increase in this value, the total users decreases by `r model2$coefficients[['Humidity']]` as this is negatively correlated.

**Comment on p-value**  
All of the independent variables chosen so far are statistically significant in their relationship with the dependent variable. In other words, we reject the null hypothesis in favor of the alternate hypothesis concluding that there is strong correlation between the independent and dependent variables. It makes sense why there is also a jump in Multiple $R^2$ value.  

**Comment on Multiple R squared**  
Compared to the previous model, the Multiple $R^2$ has some improvement, which is intuitive, as we have included a second variable that is highly correlated with the dependent variable Total.Users and also has statistical significance represented by code $***$.  


### Question 8  
**We will try one more time as in the previous question, to add a third variable in our model.**  
**Temperature.Feels+Humidity vs Wind.Speed**  
```{r vif, results='asis'}
vif_values <- ezids::xkablevif(lm(Total.Users ~ Temperature.Feels.F+Humidity+Wind.Speed, data=bike_final))
vif_values
```

**Temperature.Feels+Humidity vs Temperature.F**  
```{r, results='asis'}

vif_values <- ezids::xkablevif(lm(Total.Users ~ Temperature.Feels.F+Humidity+Temperature.F, data=bike_final))
vif_values
```
Very obvious, and makes a lot sense to see Temperature.F and Temperature.Feels.F being highly correlated. Hence, Wind.Speed is chosen as the third variable for the third model.  

```{r model3, results='markup'}
model3 <- lm(Total.Users ~ Temperature.Feels.F+Humidity+Wind.Speed, data=bike_final)
summary(model3)
```
**Coefficient interpretation**  
1. An intercept value of `r model3$coefficients[['(Intercept)']]` represents the average number of users when the feel like temperature and wind speed are both $0^{\circ}F$.   
2. The estimated coefficient value for Temperature.Feels.F represents for every increase in this value, the total users increases by `r model3$coefficients[['Temperature.Feels.F']]`  
3. The estimated coefficient value for Humidity represents for every increase in this value, the total users decreases by `r model3$coefficients[['Humidity']]` as this is negatively correlated with variable Total.Users.  
4. The estimated coefficient value for Wind.Speed represents for every increase in this value, the total users decreases by `r model3$coefficients[['Wind.Speed']]` as this is negatively correlated with variable Total.Users.  

**Comment on p-value**  
All of the independent variables chosen so far are statistically significant in their relationship with the dependent variable. In other words, we reject the null hypothesis in favor of the alternate hypothesis. It makes sense why there is also a jump in Multiple $R^2$ value.  

**Comment on Multiple R squared**  
Compared to the previous model, the Multiple $R^2$ has the slightest of improvement as the p value for Wind.Speed is 0.036, which is closer to 0.05 at 95% confidence. This means the variable is less significant to the response variable in the model and is also represented by the significance code $*$.  

### Question 9  
**For the 3-variable model you found, find the confidence intervals of the coefficients.**  
```{r, results='markup'}
stats::confint(model3)
```
```{r delete, include=F}
critical_value <- qnorm(0.975)
estimate <- 3.823
std_error <- 0.217

lower_bound <- estimate-critical_value*std_error
upper_bound <- estimate+critical_value*std_error
print(lower_bound)
print(upper_bound)
```
### Question 10    
**Use ANOVA to compare the three different models you found.**  
You have found three different models. Use ANOVA test to compare their residuals. What conclusion can you draw?

```{r anova, results='markup'}
a_variance_analysis <- anova(model1, model2, model3)
a_variance_analysis
```

As already commented for Model3, there is not much improvement from Model2 to Model3 as the p value is 0.036 that is somewhat closer to 0.05 at 95% confidence level. However, it is still considered significant as the p value is lesser than 0.05.  
From Model1 to Model2, there is certainly substantial statistical significance as the code is $***$  
P.S. ANOVA on three different models compares their residuals to determine the statistical significance.  




